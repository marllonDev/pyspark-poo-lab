# Projeto PySpark - Relat√≥rio de Pedidos com Pagamentos Recusados

## Descri√ß√£o
Este projeto PySpark gera um relat√≥rio de pedidos de venda com pagamentos recusados e classificados como leg√≠timos, para o ano de 2025.

## Funcionalidades
- Filtra pedidos com pagamentos recusados (`status=false`) e leg√≠timos (`fraude=false`)
- Processa apenas pedidos do ano de 2025
- Ordena por estado, forma de pagamento e data
- Gera relat√≥rio em formato Parquet

## Estrutura do Projeto
```
pyspark-poo-lab/
‚îú‚îÄ‚îÄ src/                    # C√≥digo fonte
‚îÇ   ‚îú‚îÄ‚îÄ config/            # Configura√ß√µes
‚îÇ   ‚îú‚îÄ‚îÄ session/           # Gerenciamento de sess√£o Spark
‚îÇ   ‚îú‚îÄ‚îÄ data_io/           # Leitura e escrita de dados
‚îÇ   ‚îú‚îÄ‚îÄ business/         # L√≥gica de neg√≥cio
‚îÇ   ‚îú‚îÄ‚îÄ orchestration/    # Orquestra√ß√£o do pipeline
‚îÇ   ‚îî‚îÄ‚îÄ main.py          # Ponto de entrada
‚îú‚îÄ‚îÄ tests/                # Testes unit√°rios
‚îú‚îÄ‚îÄ data/                 # Dados de entrada e sa√≠da
‚îÇ   ‚îú‚îÄ‚îÄ input/            # Dados de entrada
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pagamentos/   # Arquivos JSON de pagamentos
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pedidos/      # Arquivos CSV de pedidos
‚îÇ   ‚îî‚îÄ‚îÄ output/           # Dados de sa√≠da
‚îú‚îÄ‚îÄ venv/                 # Ambiente virtual
‚îú‚îÄ‚îÄ pyproject.toml        # Configura√ß√£o do projeto
‚îú‚îÄ‚îÄ requirements.txt      # Depend√™ncias
‚îî‚îÄ‚îÄ README.md            # Documenta√ß√£o
```

## Pr√©-requisitos
- Python 3.8+
- PySpark 3.4.0+
- Pandas 1.5.0+
- Java 8+ (recomendado Java 17)

## Instala√ß√£o
```bash
# 1. Criar o ambiente virtual (primeira vez)
python3 -m venv venv

# 2. Ativar o ambiente virtual
source venv/bin/activate

# 3. Instalar depend√™ncias
pip install -r requirements.txt
```

## Execu√ß√£o
```bash
# Ativar o ambiente virtual (deve ter sido criado no passo de instala√ß√£o)
source venv/bin/activate

# Executar o pipeline completo
python3 src/main.py
```

## Testes
```bash
# Ativar o ambiente virtual (deve ter sido criado no passo de instala√ß√£o)
source venv/bin/activate

# Executar testes unit√°rios
python3 -m pytest tests/test_order_processor.py -v
```

## Configura√ß√£o
As configura√ß√µes do Spark podem ser ajustadas em `src/config/spark_config.py`.

### Configura√ß√µes Spark Ativas
- **Adaptive Query Execution**: Habilitado para otimiza√ß√£o autom√°tica
- **Partition Coalescing**: Reduz automaticamente o n√∫mero de parti√ß√µes quando ben√©fico
- **Skew Join**: Otimiza joins com dados desbalanceados
- **Modo de Execu√ß√£o**: Local com todos os cores dispon√≠veis (`local[*]`)

## Estrutura dos Dados

> **üìã Nota**: Os datasets j√° est√£o inclu√≠dos no reposit√≥rio em `data/input/`. N√£o √© necess√°rio baix√°-los separadamente.

### Dataset de Pagamentos
- **Formato**: JSON comprimido (*.json.gz)
- **Caminho**: `data/input/pagamentos/`
- **Schema**: 
  - `id_pedido` (string): Identificador do pedido
  - `forma_pagamento` (string): Forma de pagamento utilizada
  - `valor_pagamento` (double): Valor do pagamento
  - `status` (boolean): Status do pagamento (true=aprovado, false=recusado)
  - `data_processamento` (string): Data de processamento do pagamento (formato ISO)
  - `avaliacao_fraude` (object): Objeto contendo:
    - `fraude` (boolean): Indicador de fraude (true=fraudulento, false=leg√≠timo)
    - `score` (double): Score de risco de fraude

### Dataset de Pedidos
- **Formato**: CSV comprimido (*.csv.gz) com separador `;` (ponto e v√≠rgula)
- **Caminho**: `data/input/pedidos/`
- **Schema**:
  - `ID_PEDIDO` (string): Identificador √∫nico do pedido
  - `PRODUTO` (string): Nome do produto
  - `VALOR_UNITARIO` (double): Valor unit√°rio do produto
  - `QUANTIDADE` (long): Quantidade do produto
  - `DATA_CRIACAO` (timestamp): Data de cria√ß√£o do pedido (formato ISO: yyyy-MM-ddTHH:mm:ss)
  - `UF` (string): Estado onde foi realizado o pedido (c√≥digo de 2 letras)
  - `ID_CLIENTE` (long): Identificador do cliente

## Sa√≠da
O relat√≥rio √© gerado em formato Parquet no diret√≥rio `data/output/relatorio_pedidos/`:
- **Schema de Sa√≠da**:
  - `id_pedido` (string): Identificador do pedido
  - `estado` (string): Estado (UF) onde o pedido foi feito
  - `forma_pagamento` (string): Forma de pagamento
  - `valor_total` (double): Valor total do pedido (VALOR_UNITARIO √ó QUANTIDADE)
  - `data_pedido` (timestamp): Data do pedido

### Filtros Aplicados
- ‚úÖ Pedidos com pagamentos recusados (`status = false`)
- ‚úÖ Pedidos classificados como leg√≠timos (`avaliacao_fraude.fraude = false`)
- ‚úÖ Apenas pedidos do ano de 2025
- ‚úÖ Ordena√ß√£o: estado ‚Üí forma_pagamento ‚Üí data_pedido

## Status do Projeto

### ‚úÖ Testes Realizados
- **Testes Unit√°rios**: ‚úÖ PASSOU - Todas as funcionalidades da classe `OrderProcessor` testadas (1 teste executado em ~5s)
- **Pipeline Completo**: ‚úÖ PASSOU - Pipeline executado com sucesso nos dados reais 
- **Leitura de Dados**: ‚úÖ PASSOU - Datasets de pagamentos (JSON.gz) e pedidos (CSV.gz) lidos corretamente
- **Estrutura de Dados**: ‚úÖ PASSOU - Schema de entrada e sa√≠da validados

### üìä Resultados da √öltima Execu√ß√£o
- **Data da Execu√ß√£o**: 30 de Agosto de 2025
- **Filtros aplicados**: Pagamentos recusados (`status=false`) e leg√≠timos (`avaliacao_fraude.fraude=false`) do ano 2025
- **Processamento**: Join entre datasets, c√°lculo de valor total, ordena√ß√£o por estado/forma_pagamento/data
- **Arquivo de sa√≠da**: `data/output/relatorio_pedidos/part-*.snappy.parquet` (~28KB)
- **Status**: ‚úÖ Pipeline executado com sucesso
- **Formato de sa√≠da**: Parquet com compress√£o Snappy

### üîß Arquitetura Implementada
- ‚úÖ **Orienta√ß√£o a Objetos**: Todas as classes implementadas
- ‚úÖ **Inje√ß√£o de Depend√™ncias**: Configurada no `main.py`
- ‚úÖ **Configura√ß√µes Centralizadas**: `SparkConfig`
- ‚úÖ **Gerenciamento de Sess√£o**: `SparkSessionManager`
- ‚úÖ **Separa√ß√£o de Responsabilidades**: Reader, Writer, Processor, Orchestrator
- ‚úÖ **Logging**: Configurado em todas as classes
- ‚úÖ **Tratamento de Erros**: Try/catch implementado
- ‚úÖ **Empacotamento**: pyproject.toml, requirements.txt, MANIFEST.in

## Troubleshooting

### ‚ö†Ô∏è Problemas Comuns

1. **Erro de Ambiente Virtual**: 
   ```bash
   # Se o venv n√£o existir ou estiver corrompido, recrie
   rm -rf venv
   python3 -m venv venv
   source venv/bin/activate
   pip install -r requirements.txt
   ```

2. **Paths ou Diret√≥rio de Execu√ß√£o**: 
   - Execute sempre a partir do diret√≥rio raiz do projeto (`pyspark-poo-lab/`)
   - N√£o execute de dentro de subpastas como `src/`

3. **Depend√™ncias ou Java**: 
   ```bash
   # Verificar Java (necess√°rio para PySpark)
   java -version
   
   # Reinstalar depend√™ncias Python
   pip install -r requirements.txt
   ```

4. **Warnings do Spark**: 
   - Os warnings sobre "native-hadoop library" e "metadata directory" s√£o normais e n√£o afetam o funcionamento
   - O pipeline funciona corretamente mesmo com esses warnings

5. **Dados n√£o encontrados**: 
   - Os datasets est√£o inclu√≠dos no reposit√≥rio em `data/input/`
   - Verifique se os arquivos `.gz` est√£o presentes nas pastas `pagamentos/` e `pedidos/`

## Autor
Eduardo Castilho de Almeida Prado - RM: 358966 <br>
Marllon Zucolotto de Almeida - RM: 358117 <br>
Mateus Bonacina Zanguettin - RM: 358472 <br>
Tiago Bento Amado - RM: 359183


## Licen√ßa
Este projeto est√° sob a licen√ßa MIT.
