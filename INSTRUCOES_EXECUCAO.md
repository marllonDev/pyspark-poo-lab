# ğŸ“‹ InstruÃ§Ãµes de ExecuÃ§Ã£o - Projeto PySpark

## ğŸ¯ Objetivo do Projeto
Gerar um relatÃ³rio de pedidos de venda com pagamentos recusados (`status=false`) e classificados como legÃ­timos (`fraude=false`), apenas para o ano de 2025, ordenado por estado, forma de pagamento e data.

## ğŸ—ï¸ Estrutura Implementada

### âœ… CritÃ©rios Atendidos
- [x] **Schemas explÃ­citos** - Definidos em `DataReader`
- [x] **OrientaÃ§Ã£o a objetos** - Todas as classes implementadas
- [x] **InjeÃ§Ã£o de DependÃªncias** - Implementada no `main_project.py`
- [x] **ConfiguraÃ§Ãµes centralizadas** - Classe `SparkConfig`
- [x] **SessÃ£o Spark gerenciada** - Classe `SparkSessionManager`
- [x] **Leitura e escrita de dados** - Classes `DataReader` e `DataWriter`
- [x] **LÃ³gica de negÃ³cio** - Classe `OrderProcessor`
- [x] **OrquestraÃ§Ã£o do pipeline** - Classe `PipelineOrchestrator`
- [x] **Logging configurado** - Implementado em todas as classes
- [x] **Tratamento de erros** - Try/catch em todas as operaÃ§Ãµes
- [x] **Empacotamento da aplicaÃ§Ã£o** - pyproject.toml, requirements.txt, MANIFEST.in
- [x] **Testes unitÃ¡rios** - Implementados para `OrderProcessor`

## ğŸš€ Passos para ExecuÃ§Ã£o

### 1. PreparaÃ§Ã£o do Ambiente

#### 1.1 Verificar Python
```bash
python --version
# ou
python3 --version
```

#### 1.2 Criar ambiente virtual (recomendado)
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

#### 1.3 Instalar dependÃªncias
```bash
pip install -r requirements.txt
```

### 2. ObtenÃ§Ã£o dos Datasets

#### 2.1 Dataset de Pagamentos
```bash
# Clonar repositÃ³rio
git clone https://github.com/infobarbosa/dataset-json-pagamentos.git

# Copiar dados para a estrutura do projeto
cp -r dataset-json-pagamentos/data/pagamentos/* data/input/pagamentos/
```

#### 2.2 Dataset de Pedidos
```bash
# Clonar repositÃ³rio
git clone https://github.com/infobarbosa/datasets-csv-pedidos.git

# Copiar dados para a estrutura do projeto
cp -r datasets-csv-pedidos/data/pedidos/* data/input/pedidos/
```

### 3. ValidaÃ§Ã£o do Projeto

#### 3.1 Executar script de validaÃ§Ã£o
```bash
python test_project_structure.py
```

Este script irÃ¡ verificar:
- âœ… Estrutura de diretÃ³rios
- âœ… Arquivos de cÃ³digo fonte
- âœ… Arquivos de configuraÃ§Ã£o
- âœ… Dados de entrada

### 4. ExecuÃ§Ã£o dos Testes

#### 4.1 Executar testes unitÃ¡rios
```bash
pytest tests/ -v
```

#### 4.2 Verificar cobertura (opcional)
```bash
pytest tests/ --cov=src --cov-report=html
```

### 5. ExecuÃ§Ã£o do Pipeline

#### 5.1 Executar aplicaÃ§Ã£o principal
```bash
python src/main_project.py
```

#### 5.2 Verificar logs
A aplicaÃ§Ã£o irÃ¡ mostrar logs detalhados de cada etapa:
- ConfiguraÃ§Ã£o do Spark
- Leitura dos dados
- Processamento
- Escrita do resultado

### 6. VerificaÃ§Ã£o dos Resultados

#### 6.1 Verificar arquivo de saÃ­da
```bash
ls data/output/relatorio_pedidos/
```

#### 6.2 Visualizar dados (opcional)
```python
from pyspark.sql import SparkSession

# Criar sessÃ£o Spark
spark = SparkSession.builder.appName('Verificacao').getOrCreate()

# Ler resultado
df = spark.read.parquet('data/output/relatorio_pedidos')

# Mostrar dados
df.show(10)
df.printSchema()

# Contar registros
print(f"Total de registros: {df.count()}")

# Verificar filtros
df.filter("estado = 'SP'").show(5)
```

## ğŸ“Š Estrutura dos Dados

### Dataset de Pagamentos (JSON)
- **Caminho**: `data/input/pagamentos/*.json.gz`
- **Schema**:
  - `id_pedido` (String)
  - `status` (Boolean)
  - `fraude` (Boolean)
  - `forma_pagamento` (String)
  - `valor` (Double)
  - `data_pagamento` (Timestamp)

### Dataset de Pedidos (CSV)
- **Caminho**: `data/input/pedidos/*.csv.gz`
- **Schema**:
  - `id_pedido` (String)
  - `estado` (String)
  - `forma_pagamento` (String)
  - `valor_total` (Double)
  - `data_pedido` (Timestamp)

### RelatÃ³rio de SaÃ­da (Parquet)
- **Caminho**: `data/output/relatorio_pedidos/`
- **Colunas**:
  - `id_pedido`
  - `estado`
  - `forma_pagamento`
  - `valor_total`
  - `data_pedido`

## ğŸ”§ ConfiguraÃ§Ãµes

### Spark Config
As configuraÃ§Ãµes do Spark podem ser ajustadas em `src/config/spark_config.py`:

```python
@dataclass
class SparkConfig:
    app_name: str = "RelatorioPedidos"
    master: str = "local[*]"
    spark_configs: Dict[str, Any] = None
```

### Logging
O logging estÃ¡ configurado para mostrar:
- Timestamps
- NÃ­veis de log (INFO, ERROR)
- Mensagens detalhadas de cada etapa

## ğŸ› SoluÃ§Ã£o de Problemas

### Erro de MemÃ³ria
```python
# Ajustar em spark_config.py
spark_configs = {
    "spark.sql.adaptive.enabled": "true",
    "spark.sql.adaptive.coalescePartitions.enabled": "true",
    "spark.sql.adaptive.skewJoin.enabled": "true",
    "spark.driver.memory": "2g",
    "spark.executor.memory": "2g"
}
```

### Erro de Schema
- Verificar se os schemas em `DataReader` estÃ£o corretos
- Validar formato dos dados de entrada
- Usar `mode="PERMISSIVE"` para leitura tolerante

### Erro de DependÃªncias
```bash
# Reinstalar dependÃªncias
pip uninstall pyspark pandas pytest
pip install -r requirements.txt
```

## ğŸ“ˆ Monitoramento

### Logs de ExecuÃ§Ã£o
A aplicaÃ§Ã£o gera logs detalhados:
```
2025-08-27 20:30:15 - INFO - Iniciando aplicaÃ§Ã£o
2025-08-27 20:30:16 - INFO - Lendo dataset de pagamentos
2025-08-27 20:30:17 - INFO - Lendo dataset de pedidos
2025-08-27 20:30:18 - INFO - Processando dados
2025-08-27 20:30:19 - INFO - Escrevendo relatÃ³rio
2025-08-27 20:30:20 - INFO - Pipeline executado com sucesso!
```

### MÃ©tricas de Performance
- Tempo de execuÃ§Ã£o por etapa
- NÃºmero de registros processados
- Uso de memÃ³ria

## ğŸ“ EvidÃªncias para Entrega

### Screenshots NecessÃ¡rios
1. **Estrutura do projeto** - `tree /F` ou `ls -la`
2. **ExecuÃ§Ã£o do script de validaÃ§Ã£o** - `python test_project_structure.py`
3. **Resultado dos testes** - `pytest tests/ -v`
4. **ExecuÃ§Ã£o do pipeline** - `python src/main_project.py`
5. **Arquivo de saÃ­da** - `ls data/output/relatorio_pedidos/`
6. **CÃ³digo fonte das classes principais** (mÃ¡ximo 20 linhas cada)

### Dicas para Screenshots
- Use fonte legÃ­vel (12pt ou maior)
- Capture apenas as partes relevantes
- Inclua timestamps nos logs
- Mostre a estrutura completa de diretÃ³rios
- Verifique se os dados de saÃ­da estÃ£o corretos

## ğŸ¯ Checklist Final

### Antes da Entrega
- [ ] Todos os testes passando
- [ ] Pipeline executando sem erros
- [ ] Arquivo de saÃ­da gerado corretamente
- [ ] Logs mostrando todas as etapas
- [ ] Screenshots das evidÃªncias capturados
- [ ] RepositÃ³rio no GitHub criado
- [ ] DocumentaÃ§Ã£o completa

### CritÃ©rios de AvaliaÃ§Ã£o
- [ ] Schemas explÃ­citos âœ…
- [ ] OrientaÃ§Ã£o a objetos âœ…
- [ ] InjeÃ§Ã£o de dependÃªncias âœ…
- [ ] ConfiguraÃ§Ãµes centralizadas âœ…
- [ ] SessÃ£o Spark gerenciada âœ…
- [ ] Leitura e escrita de dados âœ…
- [ ] LÃ³gica de negÃ³cio âœ…
- [ ] OrquestraÃ§Ã£o do pipeline âœ…
- [ ] Logging configurado âœ…
- [ ] Tratamento de erros âœ…
- [ ] Empacotamento da aplicaÃ§Ã£o âœ…
- [ ] Testes unitÃ¡rios âœ…

## ğŸ“ Suporte
Para dÃºvidas sobre o escopo e execuÃ§Ã£o:
- Email: profmarcelo.barbosa@fiap.com.br
- RepositÃ³rio de referÃªncia: https://github.com/infobarbosa/pyspark-poo

---

**ğŸ‰ Projeto implementado com sucesso seguindo todos os critÃ©rios especificados!**
